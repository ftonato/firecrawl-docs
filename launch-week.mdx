---
title: Launch Week II (New)
description: "Check out what's new coming to Firecrawl in Launch Week II (Oct 28th - Nov 3rd)"
og:title: "Launch Week II | Firecrawl"
og:description: "Check out what's new coming to Firecrawl in Launch Week II (Oct 28th - Nov 3rd)"
---

import BatchScrapePython from "/snippets/v1/batch-scrape/base/python.mdx";
import BatchScrapeNode from "/snippets/v1/batch-scrape/base/js.mdx";
import BatchScrapeCURL from "/snippets/v1/batch-scrape/base/curl.mdx";
import BatchScrapeOutput from "/snippets/v1/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/v1/batch-scrape/base/async-output.mdx";

## Day 1 - Batch Scrape

You can now scrape multiple URLs at the same time with our new batch endpoint. Ideal for when you don't need the scraping results immediately.

### How it works

It is very similar to how the `/crawl` endpoint works. It submits a batch scrape job and returns a job ID to check the status of the batch scrape. 

The sdk provides 2 methods, synchronous and asynchronous. The synchronous method will return the results of the batch scrape job, while the asynchronous method will return a job ID that you can use to check the status of the batch scrape.

### Usage

<CodeGroup>

<BatchScrapePython />
<BatchScrapeNode />
<BatchScrapeCURL />

</CodeGroup>

### Response

If youâ€™re using the sync methods from the SDKs, it will return the results of the batch scrape job. Otherwise, it will return a job ID that you can use to check the status of the batch scrape.

#### Synchronous

<BatchScrapeOutput />

#### Asynchronous

You can then use the job ID to check the status of the batch scrape by calling the `/batch/scrape/{id}` endpoint. This endpoint is meant to be used while the job is still running or right after it has completed **as batch scrape jobs expire after 24 hours**.

<BatchScrapeAsyncOutput />