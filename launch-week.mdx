---
title: Launch Week II (New)
description: "Check out what's new coming to Firecrawl in Launch Week II (Oct 28th - Nov 3rd)"
og:title: "Launch Week II | Firecrawl"
og:description: "Check out what's new coming to Firecrawl in Launch Week II (Oct 28th - Nov 3rd)"
---

import BatchScrapePython from "/snippets/v1/batch-scrape/base/python.mdx";
import BatchScrapeNode from "/snippets/v1/batch-scrape/base/js.mdx";
import BatchScrapeCURL from "/snippets/v1/batch-scrape/base/curl.mdx";
import BatchScrapeOutput from "/snippets/v1/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/v1/batch-scrape/base/async-output.mdx";

import ScrapeLocationPython from "/snippets/v1/scrape/location/python.mdx";
import ScrapeLocationNode from "/snippets/v1/scrape/location/js.mdx";
import ScrapeLocationCURL from "/snippets/v1/scrape/location/curl.mdx";


## Day 3 - Credit Packs

Credit Packs allow you to you can easily top up your plan if your running low.
Additionally, we now offer Auto Recharge, which automatically recharges your account when you're approaching your limit.
To enable visit the pricing page at [https://www.firecrawl.dev/pricing](https://www.firecrawl.dev/pricing)

Here are the details:

### Credit Packs 
Flexible monthly credit boosts for your projects.
- **$9/mo for 1000 credits**
- Add to any existing plan
- Choose the amount you need

### Auto Recharge Credits
Automatically top up your account when credits run low.
- **$11 per 1000 credits**
- Enable auto recharge with any subscription plan




## Day 2 - Geolocation

Introducing location and language settings for scraping requests. Specify country and preferred languages to get relevant content based on your target location and language preferences.

### How it works

When you specify the location settings, Firecrawl will use an appropriate proxy if available and emulate the corresponding language and timezone settings. By default, the location is set to 'US' if not specified.

### Usage

To use the location and language settings, include the `location` object in your request body with the following properties:

- `country`: ISO 3166-1 alpha-2 country code (e.g., 'US', 'AU', 'DE', 'JP'). Defaults to 'US'.
- `languages`: An array of preferred languages and locales for the request in order of priority. Defaults to the language of the specified location.

<CodeGroup>
<ScrapeLocationPython />
<ScrapeLocationNode />
<ScrapeLocationCURL />
</CodeGroup>

## Day 1 - Batch Scrape

You can now scrape multiple URLs at the same time with our new batch endpoint. Ideal for when you don't need the scraping results immediately.

### How it works

It is very similar to how the `/crawl` endpoint works. It submits a batch scrape job and returns a job ID to check the status of the batch scrape. 

The sdk provides 2 methods, synchronous and asynchronous. The synchronous method will return the results of the batch scrape job, while the asynchronous method will return a job ID that you can use to check the status of the batch scrape.

### Usage

<CodeGroup>

<BatchScrapePython />
<BatchScrapeNode />
<BatchScrapeCURL />

</CodeGroup>

### Response

If youâ€™re using the sync methods from the SDKs, it will return the results of the batch scrape job. Otherwise, it will return a job ID that you can use to check the status of the batch scrape.

#### Synchronous

<BatchScrapeOutput />

#### Asynchronous

You can then use the job ID to check the status of the batch scrape by calling the `/batch/scrape/{id}` endpoint. This endpoint is meant to be used while the job is still running or right after it has completed **as batch scrape jobs expire after 24 hours**.

<BatchScrapeAsyncOutput />